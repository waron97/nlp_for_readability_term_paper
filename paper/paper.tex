\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{graphicx}

\usepackage[]{acl}
\usepackage{tabularx}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{microtype}

\title{Examining readability of text generated through GPT-3 }

\author{Aron Winkler \\
  University of TÃ¼bungen \\
  \texttt{winkler.aron5@gmail.com}}

\begin{document}
\maketitle
\begin{abstract}
    This paper examines readability-related metrics of AI-generated text, specifically texts produced by OpenAI's GPT-3. Generation was performed for the English language across 10 topics and the 6 proficiency levels defined by the CEFR framework, and outputs were compared with Simple Wikipedia and (standard) Wikipedia articles on the same topics. 7 commonly used complexity metrics have been employed, targeting chiefly lexis and syntax. In addition, a classifier is also trained on Wikipedia data and used to evaluate the generated texts. In the limited scope of this experiment, GPT-3 is observed to be at least partially aware of what constitutes reading difficulty.
\end{abstract}

\section{Introduction}

AI text generation has experiences a massive rise in popularity with the media reach achieved by the release of OpenAI's GPT-3 \citep{brown2020language} and its chat GUI interface ChatGPT\footnote{\url{https://openai.com/blog/chatgpt}}. While early public opinion focused on how - in the school setting - it would be students who would benefit from the new system by having their schoolwork done for them, little was said about how AI text generation could become an asset for didactitians in generating teaching material.

AI text generation might become an important tool for teachers, allowing them to generate learning material about topics and events that engage their students that might not be available on the market. An interesting setting for such a phenomenon is language learning, where it is necessary to produce learning material for students at different proficiency levels: whether to create ex-novo or simplify existing text material, AI text-generation tools can easily be imagined as having an enabling effect. 

It is however not obvious that AI generation tools reach the desired quality or adherence to the didactic goals that teachers set out. Owing to the increased popularity of these tools, more research is desirable on their applicability in a number of different settings. For example, GPT-3 has been observed to "make up" information about some real-world events or phenomena, a fact that makes it hard to rely on for teaching. In the specific context of this experiment, employing GPT-3 for language-learning material is only valuable insofar as the generated content is adequate in difficulty to the audience.

This work aims to make a contribution to these questions by verifying the readability level of text content generated by GPT-3 with regard to 10 topics and the 6 proficiency levels defined by the CEFR framework. A number of complexity metrics are computed on the generated texts and compared with Simple Wikipedia\footnote{\url{https://simple.wikipedia.org/wiki/Main_Page}} as well as Wikipedia\footnote{\url{https://www.wikipedia.org/}} articles on the same topics in order to gauge the true readability levels of the model outputs.
\section{Dataset and methods}
\subsection{Dataset}

OpenAI's REST api was employed to produce texts with the "text-davinci-003" model (a GPT-3 variant) across 10 topics\footnote{'Color Blindness, The Great Depression, Butterflies, Dogs, Semantics, The Internet, The Moon, Dinosaurs, Economics, Quantum Mechanics'} and 6 CEFR proficiency levels\footnote{A1, A2, B1, B2, C1, C2}. Topics were selected without any particular guiding principle other than a general goal of variety between events, entities, and phenomena. The prompts fed to the model followed the template:

\begin{quote}
  \emph{Write a text for learners of English at the [level] level about [topic].}
\end{quote}

In addition to the 60 texts obtained in this way, the dataset was enriched by Simple Wikipedia and Wikipedia articles corresponding to each of the 10 topics, resulting in an additional 20 dataset entries.

\subsection{Preprocessing}

Preprocessing of the dataset comprised enrichment with POS and dependency annotation. Part-of-speech tagging was achieved with nltk \citep{LoperBird02} and dependency relations were added through nltk in conjunction with CoreNLP models \citep{corenlp}.

\subsection{Metrics}

\input{tables/metrics.tex}

Table~\ref{tab:metrics} lists the readability metrics used in this experiment. This specific selection was inspired by \citep{venturi-etal-2015-nlp}, but many more examples of their usage can be found. 

This feature set almost exclusively targets lexis and syntax, with little regard for morphology outside of the noun-to-verb ratio feature. While this is a limitation of this set, the language of this experiment is solely English, a language not necessarily known for a complex morphological system.

The feature "Words in Top 3000 English words" uses unigram frequencies from the Google Web Trillion Word corpus\footnote{\url{https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html}} to isolate the 3000 most frequent words of the English language. Many research approaches to readability employ similar ideas to gain insight into the lexical complexity of texts, such as the BIV (basic Italian vocabulary) employed by \citep{dellorletta-etal-2011-read}.

All feature values outside of type-token-ratio (which ignores sentence boundaries) are computed at the sentence level. When a dataset entry comprises multiple sentences, the output for that entry is the average of the values for the sentences that compose it. 

\subsection{Text classifier}

A classifier was trained on Wikipedia and Simple Wikipedia data, specifically on a train dataset of approximately 240k paragraphs, evenly distributed across sourced from Wikipedia and Simple Wikipedia articles. A test set of 40k entries, sourced and distributed in the same manner as the train set, was also obtained to evaluate the data. The task of the classifier was to determine whether a given paragraph had originally been a part of Simple Wikipedia or (standard) Wikipedia - $P(standard)$ is the derived readability metric reported in the results.

The model takes BERT sentence embeddings as input, which are fed through a single-layer LSTM with hidden size 126. LSTM outputs are then used to compute a distribution over the two labels in the dataset, namely "simple" (paragraph obtained from Simple Wikipedia) and "standard" (paragraph obtained from standard Wikipedia). Training was performed with the Adam optimizer and a batch size of 50 for 5 epochs. After evaluating the model at each epoch, it was observed that after the third epoch there are signs of overfitting, therefore the final parameters were those obtained after 3 epochs of training. Hardware-wise, the free tier of Google Colab was used - do to time limits set on the platform, several days were necessary for the full training process. 
After the third epoch, the model achieves 87.8\% macro-averaged $F_1$-score on the test set. While this is not state-of-the-art performance, it is likely good enough for the scope of this project.

\section{Results}

\subsection{Metrics}

\input{tables/means.tex}

Table~\ref{tab:means} details the average output for each difficulty level and metric. For each metric, the 3 values most indicative of low readability are highlighted.

With regards to the selected metrics, GPT-3 does a good job generating texts for the target readability level in the sense that prompts requesting C1 and C2 level material on average yield the least readable texts, whereas lower proficiency levels tend to have scores indicating higher readability.
 
While no rigorous checks are carried out in this paper in reference to the correlation between these metrics and article readability levels, it's still possible to examine some of the results to gain some insight into the black box that is GPT-3.

\input{figs/wit_means.tex}

Among the selected measures, WIT (ratio of words in the 3000 most frequent English words) appears to be the most well-behaved, insofar as the prompted CEFR levels mostly maintain their natural progression. Figure~\ref{fig:wit-mean} shows the isolated scores for the WIT metric, which show a correlation with difficulty. Wikipedia and Simple Wikipedia articles have lower values, however this is possibly connected to those texts being several times longer.

\input{figs/lldl_means.tex}

LLDL (length of longest dependency link) is another metric that performs well in terms of level differentiation. It also reflects the common belief that the biggest difficulty spike in language learning - as shown by Figure~\ref{fig:lldl-mean}. As with most metrics, however, the natural progression of the CEFR levels is lost, particularly in the middle portions.

Interesting discussions can be had around topics as well, since, depending on GPT-3's ability to generalize across domains, certain areas might constitute problem for it. For example, "semantics" and "quantum mechanics" would be hard to write about for A1 level students as they are likely to be children who have not interacted with those fields yet.

Generally, a difference in values is visible across the board, more so at the lower difficuly levels. At CEFR levels B2 and above, many of the metrics appear similar regardless of topic. Among the selected measures, NVR (noun to verb ratio) shows the highest cross-topic variation, maintaining strong differentitation even at the C2-level texts. Figure \ref{fig:wit-nvr-topics} shows metric values at the A1 and C2 levels, which indicate that topics commonly regarded as intrinsically have higher information density in GPT-3 texts. This is however a weak case, as few metrics differ so sharply as NVR, and bars in these figures correspond to a single text entry.

\input{figs/nvr_topics.tex}

\subsection{Classifier scores}

\input{tables/classifier_scores.tex}

Table~\ref{tab:classifier} shows classifier outputs, with highlighted values for each topic and difficulty level associated to lowest readability. The Wikipedia entries that were reported on in previous sections are not included here, as they may have been part of the train set of the classifier.

These results show again that GPT-3 has learned what constitues readability, at least to some extent. For 4 of the topics presented in this study, the C2 prompt achieves the highest complexity (or lowest readability) level, with 3 more having B2 as the top entry. 

As was observed with the metrics, there tends to be good differentiation between the extremes (i.e. between A1 and C2 texts), but this is less true in the middle portions. Interestingly, prompts for C1 level texts often do not yield particularly difficult texts according to the classifier. Such a claim is confirmed by looking at the average values for each level, where C1 does score higher than A1 and A2, but lower than B2. Whether this is caused by this classifier's failure to capture some aspect of text readability, ot by GPT-3's lessened ability to generate material for this target level, remains to be confirmed by more robust experiments.

On the matter of inherent topic complexities, there is unfortunately not much to talk about - no topic or set of topics emerge as consistently more difficult than others. However, \emph{Economics} and \emph{Semantics} do get the highest average score, indicating they are the least readable, while (more surprisingly) \emph{Color Blindness} is the most readable on average.

\section{Discussion and future research}

This work attempted to gauge true readability levels of GPT-3 output texts with prompts across 10 topics and CEFR proficiency levels, optimising for the second language learning application environment. Outputs computed from the metric set suggest that GPT-3 is at least somewhat aware of syntax and lexis when performin generation for a target readability level.

Future research into this topic could explore a wider variety of textual metrics, as this work only selected a handful that lent themselves particularly well to interpretation and did not require steep hardware requirements to obtain. 

Since recent research (e.g. \citealp{Deutsch_2020}) has also focused on readability assessment through neural approaches without such metrics, this paper also attempted to adopt this strategy by training a classifier on Wikipedia and Simple Wikipedia data. Outputs derived from this model also support the theory of GPT-3 being at least somewhat aware of readability-related aspects of language. With regards to this section, it is nonetheless paramount to utilise a larger, SOTA readability model for exploring complexity - as the one employed here is only suitable for prototyping - as well as to perform the experiment on a richer repository of GPT-3 data.

In summary, future extensions of this experiment or related experiments would most benefit from higher volume. Simply put, more data is necessary across multiple domains to make educated conclusions about the quality of GPT-3 outputs in a teaching setting. While data is easy to generate, quality annotations on it are yet unavailable. Indeed, it is a good question whether making any is a fruitful enterprise, as the actual models behind GPT-3 and ChatGPT are constantly updated - thus making it hard to pinpoint what exactly is being studied.


\section*{Acknowledgements}

I thank Prof. Dr. Walt Detmar Meurers for exposing me to the topics of readability and automatic readability assessment, and for the excellent course he taught on the matter.

\bibliography{anthology,custom}

\appendix
\section*{Resources}

\label{sec:appendix}

All code for this paper is available at \url{https://github.com/waron97/nlp_for_readability_term_paper}.

\end{document}