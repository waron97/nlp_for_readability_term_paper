{"cells":[{"cell_type":"code","execution_count":140,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680265900564,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"cQn6tPPdSXfJ"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","import torch\n","import torch.nn as nn\n","import csv\n","import numpy as np\n","import typing as t\n","import csv\n","from transformers import BertTokenizer, BertModel\n","from tqdm import tqdm\n","from sklearn.metrics import precision_recall_fscore_support\n","import json\n","import pandas as pd"]},{"cell_type":"code","execution_count":141,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680265900565,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"qjZAm7NCYBRM","outputId":"396a94f9-ede5-4a6f-b7e6-16498c42772b"},"outputs":[{"data":{"text/plain":["device(type='mps')"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cpu\")\n","\n","if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n","    device = torch.device(\"mps\")\n","\n","device"]},{"cell_type":"code","execution_count":142,"metadata":{"executionInfo":{"elapsed":1205,"status":"ok","timestamp":1680265901764,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"kVN60FyVT_kX"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',)"]},{"cell_type":"code","execution_count":143,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680265901765,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"kfVVyp9-S6y3"},"outputs":[],"source":["class WikiData(Dataset):\n","  def __init__(self, csv_path:str = \"\", limit: int = 0):\n","    self.csv_path = csv_path\n","    self.limit = limit\n","    self.data = self._load_csv()\n","\n","  def _load_csv(self):\n","    rows = []\n","    i = 0\n","    with open(self.csv_path, \"r\", encoding=\"utf-8\") as f:\n","      reader = csv.reader(f, delimiter=\",\")\n","      for row in reader:\n","        if self.limit and i >= self.limit:\n","          break\n","        _id, label, text = row\n","        rows.append((_id, label, text))\n","        i += 1\n","    return rows\n","\n","  def __len__(self):\n","    return len(self.data)\n","  \n","  def __getitem__(self, idx: int):\n","    _id, label, text = self.data[idx]\n","    label = 0 if label == \"standard\" else 1\n","    return text, label"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[],"source":["class GPT_Dataset(Dataset):\n","    def __init__(self, db_path: str) -> None:\n","        self.db_path = db_path\n","        self.data = self._load_data()\n","    \n","    def _load_data(self):\n","        \n","        with open(self.db_path, \"r\", encoding=\"utf-8\") as f:\n","            d = json.load(f)\n","            data = []\n","            data.extend(d[\"gpt_generation\"])\n","            # for item in d[\"wikipedia\"][\"standard\"]:\n","            #     data.append({**item, \"level\": \"standard\"})\n","            # for item in d[\"wikipedia\"][\"simple\"]:\n","            #     data.append({**item, \"level\": \"simple\"})\n","            return data\n","        \n","    def _unpack(self, row: dict):\n","        return row[\"topic\"], row[\"level\"], row[\"text\"]\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx: int):\n","        row = self.data[idx]\n","        topic, level, text = self._unpack(row)\n","        return topic, level, text\n","    "]},{"cell_type":"code","execution_count":145,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680265901766,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"M9vtQ-yqUXwr"},"outputs":[],"source":["class ReadabilityClassifier(nn.Module):\n","  def __init__(self,\n","               hidden_size: int = 126,\n","               n_lstm_layers: int = 1\n","               ):\n","    super().__init__()\n","\n","    self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","    for param in self.bert.parameters():\n","      param.requires_grad = False\n","\n","    self.lstm = nn.LSTM(\n","        input_size = 769,\n","        hidden_size = hidden_size,\n","        num_layers = n_lstm_layers,\n","        batch_first = True\n","    )\n","    self.linear = nn.Linear(hidden_size, 2)\n","    self.softmax = nn.LogSoftmax(dim=1)\n","  \n","  def forward(self, tokens):\n","    attention = tokens.attention_mask\n","    embedded = self.bert(**tokens).last_hidden_state\n","    attention = attention.reshape(embedded.shape[0], -1, 1)\n","    embedded = torch.cat((embedded, attention), dim=2)\n","    output, _ = self.lstm(embedded)\n","    output = output[:, -1, :]\n","    output = self.linear(output)\n","    sm = self.softmax(output)\n","    return sm\n"]},{"cell_type":"code","execution_count":146,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680265901767,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"u5RjaNTuXiII"},"outputs":[],"source":["def train(\n","    model: ReadabilityClassifier, \n","    criterion: nn.Module,\n","    dataloader: DataLoader,\n","    optimizer,\n","    n_epochs: int = 1,\n","):\n","  for epoch in range(n_epochs):\n","    loop = tqdm(dataloader)\n","    losses = []\n","    for texts, labels in loop:\n","      optimizer.zero_grad()\n","      tokens = tokenizer(\n","          texts, \n","          return_tensors=\"pt\", \n","          padding=True, \n","          truncation=True\n","      ).to(device)\n","      labels = labels.to(device)\n","      output = model(tokens)\n","      loss = criterion(output, labels)\n","      losses.append(loss.item())\n","      loss.backward()\n","      optimizer.step()\n","    print(f\"Loss at epoch {epoch}: {round(sum(losses) / len(losses), 4)}\")\n","    \n","    torch.save({\n","        \"epoch\": epoch,\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"loss\": sum(losses) / len(losses)\n","    }, f\"checkpoints/epoch_{epoch}.tar\")"]},{"cell_type":"code","execution_count":147,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680265901767,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"jbMpww3RiDOh"},"outputs":[],"source":["def evaluate(model: nn.Module, dataloader: DataLoader):\n","  y_true = []\n","  y_pred = []\n","  with torch.no_grad():\n","    loop = tqdm(dataloader)\n","    for texts, levels in loop:\n","      tokens = tokenizer(\n","          texts, \n","          return_tensors=\"pt\", \n","          padding=True, \n","          truncation=True\n","      ).to(device)\n","      levels = levels.to(device)\n","      output = model(tokens)\n","      output = output.argmax(dim=1).cpu().detach().numpy()\n","      levels = levels.cpu().detach().numpy()\n","      y_true.extend(levels)\n","      y_pred.extend(output)\n","    precision, recall, fscore, _ = precision_recall_fscore_support(\n","        y_true = levels, \n","        y_pred = output,\n","        labels=[0, 1],\n","    )\n","    print()\n","    print(\"Precision\", precision)\n","    print(\"Recall\", recall)\n","    print(\"Fscore\", fscore)\n","    return y_true, y_pred\n","      "]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":["def score_gpt_data(model: ReadabilityClassifier, dataloader: DataLoader):\n","    scores = {}\n","    for topics, levels, texts in dataloader:\n","        tokens = tokenizer(\n","            texts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True\n","        ).to(device)\n","        output = model(tokens)\n","        for topic, level, score in zip(topics, levels, output):\n","            if topic not in scores:\n","                scores[topic] = {}\n","            if level not in scores[topic]:\n","                scores[topic][level] = []\n","            p_standard = torch.exp(score).cpu().detach().numpy()[0]\n","            scores[topic][level] = round(p_standard, 3)\n","    return scores"]},{"cell_type":"code","execution_count":149,"metadata":{"executionInfo":{"elapsed":26822,"status":"ok","timestamp":1680265928581,"user":{"displayName":"Winkler Áron","userId":"05619291698364576744"},"user_tz":-120},"id":"sMEkOOdwN5PW"},"outputs":[{"name":"stdout","output_type":"stream","text":["284154\n"]}],"source":["dst = WikiData(\"./distrib/dataset_train.csv\")\n","print(len(dst))\n","\n","train_dataloader = DataLoader(\n","    WikiData(\"./distrib/dataset_train.csv\"),\n","    batch_size=50,\n","    shuffle=True\n",")"]},{"cell_type":"code","execution_count":150,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inaC96JNX0MC","outputId":"f3760bee-3853-4a5d-8a64-b0e099894f4f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":150,"metadata":{},"output_type":"execute_result"}],"source":["model = ReadabilityClassifier()\n","model.to(device)\n","criterion = nn.NLLLoss()\n","optimizer = Adam(model.parameters(), lr=2e-5)\n","checkpoint = torch.load(\"checkpoints/epoch_2.tar\", map_location=device)\n","# train(model, criterion, train_dataloader, optimizer, n_epochs=5)\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n"]},{"cell_type":"code","execution_count":151,"metadata":{"id":"4XZj_3aWis9a"},"outputs":[],"source":["# test_dataloader = DataLoader(\n","#     WikiData(\"./distrib/dataset_test.csv\"),\n","#     batch_size=50,\n","# )\n","\n","# y_true, y_pred = evaluate(model, test_dataloader)"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["gpt_dataloader = DataLoader(\n","    GPT_Dataset(\"./distrib/dataset.json\"),\n","    batch_size=10\n",")\n","\n","scores = score_gpt_data(model, gpt_dataloader)\n"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>A1</th>\n","      <th>A2</th>\n","      <th>B1</th>\n","      <th>B2</th>\n","      <th>C1</th>\n","      <th>C2</th>\n","      <th>Avg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Color Blindness</th>\n","      <td>0.02</td>\n","      <td>0.022</td>\n","      <td>0.056</td>\n","      <td>0.095</td>\n","      <td>0.015</td>\n","      <td>0.03</td>\n","      <td>0.04</td>\n","    </tr>\n","    <tr>\n","      <th>The Great Depression</th>\n","      <td>0.014</td>\n","      <td>0.128</td>\n","      <td>0.027</td>\n","      <td>0.89</td>\n","      <td>0.342</td>\n","      <td>0.461</td>\n","      <td>0.31</td>\n","    </tr>\n","    <tr>\n","      <th>Butterflies</th>\n","      <td>0.016</td>\n","      <td>0.027</td>\n","      <td>0.014</td>\n","      <td>0.041</td>\n","      <td>0.021</td>\n","      <td>0.281</td>\n","      <td>0.067</td>\n","    </tr>\n","    <tr>\n","      <th>Dogs</th>\n","      <td>0.028</td>\n","      <td>0.069</td>\n","      <td>0.013</td>\n","      <td>0.208</td>\n","      <td>0.088</td>\n","      <td>0.041</td>\n","      <td>0.075</td>\n","    </tr>\n","    <tr>\n","      <th>Semantics</th>\n","      <td>0.014</td>\n","      <td>0.144</td>\n","      <td>0.013</td>\n","      <td>0.541</td>\n","      <td>0.475</td>\n","      <td>0.262</td>\n","      <td>0.242</td>\n","    </tr>\n","    <tr>\n","      <th>The Internet</th>\n","      <td>0.012</td>\n","      <td>0.027</td>\n","      <td>0.291</td>\n","      <td>0.043</td>\n","      <td>0.031</td>\n","      <td>0.041</td>\n","      <td>0.074</td>\n","    </tr>\n","    <tr>\n","      <th>The Moon</th>\n","      <td>0.022</td>\n","      <td>0.073</td>\n","      <td>0.026</td>\n","      <td>0.029</td>\n","      <td>0.053</td>\n","      <td>0.812</td>\n","      <td>0.169</td>\n","    </tr>\n","    <tr>\n","      <th>Dinosaurs</th>\n","      <td>0.019</td>\n","      <td>0.015</td>\n","      <td>0.016</td>\n","      <td>0.272</td>\n","      <td>0.052</td>\n","      <td>0.384</td>\n","      <td>0.126</td>\n","    </tr>\n","    <tr>\n","      <th>Economics</th>\n","      <td>0.031</td>\n","      <td>0.012</td>\n","      <td>0.018</td>\n","      <td>0.344</td>\n","      <td>0.222</td>\n","      <td>0.697</td>\n","      <td>0.221</td>\n","    </tr>\n","    <tr>\n","      <th>Quantum Mechanics</th>\n","      <td>0.011</td>\n","      <td>0.024</td>\n","      <td>0.121</td>\n","      <td>0.465</td>\n","      <td>0.313</td>\n","      <td>0.628</td>\n","      <td>0.26</td>\n","    </tr>\n","    <tr>\n","      <th>Avg</th>\n","      <td>0.019</td>\n","      <td>0.054</td>\n","      <td>0.06</td>\n","      <td>0.293</td>\n","      <td>0.161</td>\n","      <td>0.364</td>\n","      <td>0.158</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         A1     A2     B1     B2     C1     C2    Avg\n","Color Blindness        0.02  0.022  0.056  0.095  0.015   0.03   0.04\n","The Great Depression  0.014  0.128  0.027   0.89  0.342  0.461   0.31\n","Butterflies           0.016  0.027  0.014  0.041  0.021  0.281  0.067\n","Dogs                  0.028  0.069  0.013  0.208  0.088  0.041  0.075\n","Semantics             0.014  0.144  0.013  0.541  0.475  0.262  0.242\n","The Internet          0.012  0.027  0.291  0.043  0.031  0.041  0.074\n","The Moon              0.022  0.073  0.026  0.029  0.053  0.812  0.169\n","Dinosaurs             0.019  0.015  0.016  0.272  0.052  0.384  0.126\n","Economics             0.031  0.012  0.018  0.344  0.222  0.697  0.221\n","Quantum Mechanics     0.011  0.024  0.121  0.465  0.313  0.628   0.26\n","Avg                   0.019  0.054   0.06  0.293  0.161  0.364  0.158"]},"execution_count":161,"metadata":{},"output_type":"execute_result"}],"source":["index = list(scores.keys()) + [\"Avg\"]\n","columns = list(scores[index[0]].keys()) + [\"Avg\"]\n","df = pd.DataFrame(index=index, columns=columns)\n","\n","for i in index:\n","    for j in columns:\n","        if i != \"Avg\" and j != \"Avg\":\n","            df.loc[i, j] = scores[i][j]\n","        \n","for i in index:\n","    df.loc[i, \"Avg\"] = round(df.loc[i, :].mean(), 3)\n","\n","for j in columns:\n","    df.loc[\"Avg\", j] = round(df.loc[:, j].mean(), 3)\n","\n","df = df.round(3)\n","\n","ltx = df.style.to_latex()\n","df"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/waron97/nlp_for_readability_term_paper/blob/master/classifier.ipynb","timestamp":1680207323249}]},"gpuClass":"standard","kernelspec":{"display_name":"term-paper-nfLWpm6z-py3.9","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
