{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sqlalchemy.orm import Session\n",
    "from src.sql.models import WikiArticle\n",
    "from src.sql import engine\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    # remove punctuation\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)\n",
    "    # remove extra spaces\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_data():\n",
    "    p = \"data/glove.6B/glove.6B.50d.txt\"\n",
    "    word2idx: Dict[str, int] = {}\n",
    "    idx2word: Dict[int, str] = {}\n",
    "    embs: List[List[float]] = []\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as file:\n",
    "        i = 0\n",
    "        for l in file:\n",
    "            l = l.split()\n",
    "            word, emb = l[0], l[1:]\n",
    "            emb = [float(x) for x in emb]\n",
    "            word2idx[word] = i\n",
    "            idx2word[i] = word\n",
    "            embs.append(emb)\n",
    "            i += 1\n",
    "        \n",
    "    \n",
    "    return word2idx, idx2word, np.array(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_ids(test_portion_size: float = 0.1):\n",
    "    with Session(engine) as session:\n",
    "        _ids = []\n",
    "        items = session.query(WikiArticle.id).limit(10_000).all()\n",
    "        for item in items:\n",
    "            _id = item[0]\n",
    "            _ids.append((_id, \"standard\"))\n",
    "            _ids.append((_id, \"simple\"))\n",
    "    train_size = 1 - test_portion_size\n",
    "    train = _ids[:int(len(_ids) * train_size)]\n",
    "    test = _ids[int(len(_ids) * train_size):]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(train_ids: List[Tuple[int, str]], glove_word2idx:Dict[str,int],  glove_idx2word: Dict[int, str]):\n",
    "    pickle_path = \"data/classifier_data/vocabulary.pkl\"\n",
    "    \n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, \"rb\") as file:\n",
    "            payload = pickle.load(file)\n",
    "            return payload[\"word2idx\"], payload[\"idx2word\"], payload[\"unk_idx\"], payload[\"pad_idx\"], payload[\"max_tokens\"]\n",
    "    \n",
    "    word2idx: Dict[str, int] = {}\n",
    "    idx2word = {}\n",
    "    \n",
    "    for idx, word in glove_idx2word.items():\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "    \n",
    "    voc = set()\n",
    "    max_tokens = 0\n",
    "    with Session(engine) as session:\n",
    "        for _id, level in tqdm(train_ids):\n",
    "            doc = session.query(WikiArticle).filter_by(id=_id).first()\n",
    "            text = doc.simple_text if level == \"simple\" else doc.standard_text\n",
    "            text = clean_str(text)\n",
    "            tokens = word_tokenize(text)\n",
    "            for token in tokens:\n",
    "                voc.add(token)\n",
    "            max_tokens = max(max_tokens, len(tokens))\n",
    "    \n",
    "    unprocessed = []        \n",
    "    for word in voc:\n",
    "        glove_idx = glove_word2idx.get(word)\n",
    "        if glove_idx is None:\n",
    "            unprocessed.append(word)\n",
    "    \n",
    "    i = len(word2idx)\n",
    "    for word in unprocessed:\n",
    "        while idx2word.get(i) is not None:\n",
    "            i += 1\n",
    "        word2idx[word] = i\n",
    "        idx2word[i] = word\n",
    "        i += 1\n",
    "    \n",
    "    unk_idx = len(word2idx)\n",
    "    while idx2word.get(unk_idx) is not None:\n",
    "        unk_idx += 1\n",
    "    unk = \"<UNK>\"\n",
    "    word2idx[unk] = unk_idx\n",
    "    idx2word[unk_idx] = unk\n",
    "    \n",
    "    pad_idx = len(word2idx)\n",
    "    while idx2word.get(pad_idx) is not None:\n",
    "        pad_idx += 1\n",
    "    pad = \"<PAD>\"\n",
    "    word2idx[pad] = pad_idx\n",
    "    idx2word[pad_idx] = pad\n",
    "    \n",
    "    pickle_payload = {\n",
    "        \"word2idx\": word2idx,\n",
    "        \"idx2word\": idx2word,\n",
    "        \"unk_idx\": unk_idx,\n",
    "        \"pad_idx\": pad_idx,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    with open(pickle_path, \"wb\") as file:\n",
    "        pickle.dump(pickle_payload, file)\n",
    "    \n",
    "    return word2idx, idx2word, unk_idx, pad_idx, max_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_vector(size:int) -> torch.Tensor:\n",
    "    return torch.rand(size, device=device, dtype=torch.float32)\n",
    "\n",
    "def get_embedding_weights(embs: np.ndarray, idx2word: Dict[int, str]):\n",
    "    embedding_dim = 50\n",
    "    vocab_size = len(idx2word)\n",
    "    weight_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "    for idx in idx2word:\n",
    "        if idx < len(embs):\n",
    "            weight_matrix[idx] = torch.tensor(embs[idx], device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            weight_matrix[idx] = random_vector(embedding_dim)\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(\n",
    "    string, \n",
    "    word2idx: Dict[str, int], \n",
    "    unk_idx: int, \n",
    "    limit_sents: int = None\n",
    "):\n",
    "    sents = sent_tokenize(string, language=\"english\")\n",
    "    if limit_sents:\n",
    "        sents = sents[:limit_sents]\n",
    "    sents = \".\".join(sents)\n",
    "    text = clean_str(sents)\n",
    "    toks = word_tokenize(text, language=\"english\")\n",
    "    tok_idx = [word2idx.get(tok, unk_idx) for tok in toks]\n",
    "    return tok_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArticleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, ids: List[Tuple[int, str]], \n",
    "        word2idx: Dict[str, int], \n",
    "        unk_tok_idx: int, \n",
    "        pad_token_idx: int,\n",
    "        pad_to_size: int = 1000,\n",
    "        sent_limit: int = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._ids = ids\n",
    "        self.word2idx = word2idx\n",
    "        self.unk_idx = unk_tok_idx\n",
    "        self.pad_size = pad_to_size\n",
    "        self.pad_idx = pad_token_idx\n",
    "        self.sent_limit = sent_limit\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._ids)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[str, str]:\n",
    "        _id, level = self._ids[index]\n",
    "        with Session(engine) as session:\n",
    "            item = session.query(WikiArticle).filter_by(id=_id).first()\n",
    "            if level == \"standard\":\n",
    "                text = item.standard_text\n",
    "            else:\n",
    "                text = item.simple_text\n",
    "        tok_idx = encode_text(\n",
    "            text, \n",
    "            self.word2idx, \n",
    "            self.unk_idx, \n",
    "            limit_sents=self.sent_limit\n",
    "        )\n",
    "        tok_idx = torch.tensor(tok_idx, device=device, dtype=torch.long)\n",
    "        \n",
    "        if len(tok_idx) < self.pad_size:\n",
    "            pad = torch.tensor([self.pad_idx] * (self.pad_size - len(tok_idx)), device=device, dtype=torch.long)\n",
    "            tok_idx = torch.cat([tok_idx, pad])\n",
    "        elif len(tok_idx) > self.pad_size:\n",
    "            tok_idx = tok_idx[:self.pad_size]\n",
    "        \n",
    "        level = torch.tensor([1], device=device) if level == \"standard\" else torch.tensor([0], device=device)\n",
    "        return tok_idx, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 64,\n",
    "        glove_embs: np.ndarray = None,\n",
    "        word2idx: Dict[str, int] = None,\n",
    "        idx2word: Dict[int, str] = None,\n",
    "        load_emb_weight: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.glove_embs = glove_embs\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        \n",
    "        self.emb_size = 50\n",
    "        self.embedding = nn.Embedding(len(idx2word), self.emb_size)\n",
    "        if load_emb_weight:\n",
    "            self._load_embedding_weight()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.emb_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden_to_output = nn.Linear(hidden_size, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def _load_embedding_weight(self):\n",
    "        weight = get_embedding_weights(self.glove_embs, self.idx2word)\n",
    "        self.embedding.load_state_dict({\"weight\": weight})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, document_batch: torch.Tensor):\n",
    "        document_batch = self.embedding(document_batch)\n",
    "        lstm_out, _ = self.lstm(document_batch)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        # lstm_out = self.dropout(lstm_out)\n",
    "        out = self.hidden_to_output(lstm_out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: Classifier, \n",
    "    train_loader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    criterion: nn.Module,\n",
    "    epochs: int = 10,\n",
    "):  \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "        pbar = tqdm(train_loader)\n",
    "        for (document_batch, level_batch) in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(document_batch)\n",
    "            # combine losses\n",
    "            loss = 0\n",
    "            for i in range(out.shape[0]):\n",
    "                prediction = out[i].unsqueeze(0)\n",
    "                correct = level_batch[i]\n",
    "                loss += criterion(prediction, correct)\n",
    "            pbar.set_description(f\"Loss: {loss.item()}\")\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Classifier, test_dataloader: DataLoader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for document_batch, level_batch in test_dataloader:\n",
    "        out = model(document_batch)\n",
    "        y_true.extend(level_batch.reshape(-1).tolist())\n",
    "        out = out.argmax(dim=1).reshape(-1).tolist()\n",
    "        y_pred.extend(out)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"Precision:\\t{precision}\\nRecall:\\t{recall}\\nF1\\t{fscore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = get_train_test_ids()\n",
    "glove_word2idx, glove_idx2word, glove_embs = load_glove_data()\n",
    "vocab_word2idx, vocab_idx2word, unk_idx, pad_idx, max_tokens = build_vocabulary(\n",
    "    train_ids, glove_word2idx, glove_idx2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = WikiArticleDataset(\n",
    "    train_ids, \n",
    "    vocab_word2idx, \n",
    "    unk_idx, pad_idx, \n",
    "    pad_to_size=max_tokens, \n",
    "    sent_limit=5\n",
    ")\n",
    "test_dataset = WikiArticleDataset(\n",
    "    test_ids, \n",
    "    vocab_word2idx, \n",
    "    unk_idx, pad_idx, \n",
    "    pad_to_size=max_tokens, \n",
    "    sent_limit=1\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/282 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model_path = \"data/classifier_data/model.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = Classifier(\n",
    "        glove_embs=glove_embs, \n",
    "        word2idx=vocab_word2idx, \n",
    "        idx2word=vocab_idx2word,\n",
    "        load_emb_weight=False\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "\n",
    "else:\n",
    "    model = Classifier(\n",
    "        glove_embs=glove_embs, \n",
    "        word2idx=vocab_word2idx, \n",
    "        idx2word=vocab_idx2word,\n",
    "        load_emb_weight=True\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss()\n",
    "    train(model, train_dataloader, optimizer, criterion, epochs=10)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision:\t0.25\n",
      "Recall:\t0.5\n",
      "F1\t0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aron/Library/Caches/pypoetry/virtualenvs/term-paper-nfLWpm6z-py3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "term-paper-nfLWpm6z-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
