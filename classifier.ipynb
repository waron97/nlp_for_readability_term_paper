{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sqlalchemy.orm import Session\n",
    "from src.sql.models import WikiArticle\n",
    "from src.sql import engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    # remove punctuation\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)\n",
    "    # remove extra spaces\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_data():\n",
    "    p = \"data/glove.6B/glove.6B.50d.txt\"\n",
    "    word2idx: Dict[str, int] = {}\n",
    "    idx2word: Dict[int, str] = {}\n",
    "    embs: List[List[float]] = []\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as file:\n",
    "        i = 0\n",
    "        for l in file:\n",
    "            l = l.split()\n",
    "            word, emb = l[0], l[1:]\n",
    "            emb = [float(x) for x in emb]\n",
    "            word2idx[word] = i\n",
    "            idx2word[i] = word\n",
    "            embs.append(emb)\n",
    "            i += 1\n",
    "        \n",
    "    \n",
    "    return word2idx, idx2word, np.array(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_ids(test_portion_size: float = 0.1):\n",
    "    with Session(engine) as session:\n",
    "        _ids = []\n",
    "        items = session.query(WikiArticle.id).limit(1000).all()\n",
    "        for item in items:\n",
    "            _id = item[0]\n",
    "            _ids.append((_id, \"standard\"))\n",
    "            _ids.append((_id, \"simple\"))\n",
    "    train_size = 1 - test_portion_size\n",
    "    train = _ids[:int(len(_ids) * train_size)]\n",
    "    test = _ids[int(len(_ids) * train_size):]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bocabulary(train_ids: List[Tuple[int, str]], glove_word2idx:Dict[str,int],  glove_idx2word: Dict[int, str]):\n",
    "    word2idx: Dict[str, int] = {}\n",
    "    idx2word = {}\n",
    "    \n",
    "    for idx, word in glove_idx2word.items():\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "    \n",
    "    voc = set()\n",
    "    max_tokens = 0\n",
    "    with Session(engine) as session:\n",
    "        for _id, level in tqdm(train_ids):\n",
    "            doc = session.query(WikiArticle).filter_by(id=_id).first()\n",
    "            text = doc.simple_text if level == \"simple\" else doc.standard_text\n",
    "            text = clean_str(text)\n",
    "            tokens = word_tokenize(text)\n",
    "            for token in tokens:\n",
    "                voc.add(token)\n",
    "            max_tokens = max(max_tokens, len(tokens))\n",
    "    \n",
    "    unprocessed = []        \n",
    "    for word in voc:\n",
    "        glove_idx = glove_word2idx.get(word)\n",
    "        if glove_idx is None:\n",
    "            unprocessed.append(word)\n",
    "    \n",
    "    i = len(word2idx)\n",
    "    for word in unprocessed:\n",
    "        while idx2word.get(i) is not None:\n",
    "            i += 1\n",
    "        word2idx[word] = i\n",
    "        idx2word[i] = word\n",
    "        i += 1\n",
    "    \n",
    "    unk_idx = len(word2idx)\n",
    "    while idx2word.get(unk_idx) is not None:\n",
    "        unk_idx += 1\n",
    "    unk = \"<UNK>\"\n",
    "    word2idx[unk] = unk_idx\n",
    "    idx2word[unk_idx] = unk\n",
    "    \n",
    "    pad_idx = len(word2idx)\n",
    "    while idx2word.get(pad_idx) is not None:\n",
    "        pad_idx += 1\n",
    "    pad = \"<PAD>\"\n",
    "    word2idx[pad] = pad_idx\n",
    "    idx2word[pad_idx] = pad\n",
    "    \n",
    "    return word2idx, idx2word, unk_idx, pad_idx, max_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_vector(size:int) -> torch.Tensor:\n",
    "    return torch.rand(size)\n",
    "\n",
    "def get_embedding_weights(embs: np.ndarray, idx2word: Dict[int, str]):\n",
    "    embedding_dim = 50\n",
    "    vocab_size = len(idx2word)\n",
    "    weight_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "    for idx in idx2word:\n",
    "        if idx < len(embs):\n",
    "            weight_matrix[idx] = torch.tensor(embs[idx])\n",
    "        else:\n",
    "            weight_matrix[idx] = random_vector(embedding_dim)\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArticleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, ids: List[Tuple[int, str]], \n",
    "        word2idx: Dict[str, int], \n",
    "        unk_tok_idx: int, \n",
    "        pad_token_idx: int,\n",
    "        pad_to_size: int = 1000,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._ids = ids\n",
    "        self.word2idx = word2idx\n",
    "        self.unk_idx = unk_tok_idx\n",
    "        self.pad_size = pad_to_size\n",
    "        self.pad_idx = pad_token_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._ids)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[str, str]:\n",
    "        _id, level = self._ids[index]\n",
    "        with Session(engine) as session:\n",
    "            item = session.query(WikiArticle).filter_by(id=_id).first()\n",
    "            if level == \"standard\":\n",
    "                text = item.standard_text\n",
    "            else:\n",
    "                text = item.simple_text\n",
    "        text = clean_str(text)\n",
    "        toks = word_tokenize(text, language=\"english\")\n",
    "        tok_idx = [self.word2idx.get(tok, self.unk_idx) for tok in toks]\n",
    "        tok_idx = torch.tensor(tok_idx)\n",
    "        \n",
    "        if len(tok_idx) < self.pad_size:\n",
    "            pad = torch.tensor([self.pad_idx] * (self.pad_size - len(tok_idx)))\n",
    "            tok_idx = torch.cat([tok_idx, pad])\n",
    "        elif len(tok_idx) > self.pad_size:\n",
    "            tok_idx = tok_idx[:self.pad_size]\n",
    "        \n",
    "        level = torch.tensor([1]) if level == \"standard\" else torch.tensor([0])\n",
    "        return tok_idx, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 100,\n",
    "        glove_embs: np.ndarray = None,\n",
    "        word2idx: Dict[str, int] = None,\n",
    "        idx2word: Dict[int, str] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.glove_embs = glove_embs\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.emb_size = 50\n",
    "        self.embedding = nn.Embedding(len(idx2word), self.emb_size)\n",
    "        self._load_embedding_weight()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.emb_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.emb_to_hidden = nn.Linear(self.emb_size, hidden_size)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden_to_output = nn.Linear(hidden_size, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def _load_embedding_weight(self):\n",
    "        weight = get_embedding_weights(self.glove_embs, self.idx2word)\n",
    "        self.embedding.load_state_dict({\"weight\": weight})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, document_batch: torch.Tensor):\n",
    "        document_batch = self.embedding(document_batch)\n",
    "        lstm_out, _ = self.lstm(document_batch)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        # lstm_out = self.dropout(lstm_out)\n",
    "        out = self.hidden_to_output(lstm_out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: Classifier, \n",
    "    train_loader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    criterion: nn.Module,\n",
    "    epochs: int = 10,\n",
    "):\n",
    "    for (document_batch, level_batch) in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(document_batch)\n",
    "        # combine losses\n",
    "        loss = 0\n",
    "        for i in range(out.shape[0]):\n",
    "            prediction = out[i].unsqueeze(0)\n",
    "            correct = level_batch[i]\n",
    "            loss += criterion(prediction, correct)\n",
    "        loss = loss / out.shape[0]\n",
    "        tqdm.set_description(f\"Loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Classifier, test_dataloader: DataLoader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for document_batch, level_batch in test_dataloader:\n",
    "            out = model(document_batch)\n",
    "            for i in range(out.shape[0]):\n",
    "                out_doc: torch.Tensor = out[i]\n",
    "                prediction = out_doc.argmax().item()\n",
    "                correct_answer = level_batch[i].item()\n",
    "                if prediction == correct_answer:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    print(f\"Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:11<00:00, 157.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ids, test_ids = get_train_test_ids()\n",
    "glove_word2idx, glove_idx2word, glove_embs = load_glove_data()\n",
    "vocab_word2idx, vocab_idx2word, unk_idx, pad_idx, max_tokens = build_bocabulary(\n",
    "    train_ids, glove_word2idx, glove_idx2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = WikiArticleDataset(train_ids, vocab_word2idx, unk_idx, pad_idx, pad_to_size=max_tokens)\n",
    "test_dataset = WikiArticleDataset(test_ids, vocab_word2idx, unk_idx, pad_idx, pad_to_size=max_tokens)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 92/180 [13:08<12:34,  8.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()\n\u001b[1;32m      9\u001b[0m evaluate(model, test_dataloader)\n\u001b[0;32m---> 10\u001b[0m train(model, train_dataloader, optimizer, criterion)\n\u001b[1;32m     11\u001b[0m evaluate(model, test_dataloader)\n",
      "Cell \u001b[0;32mIn[202], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(prediction, correct)\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m out\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/term-paper-nfLWpm6z-py3.9/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/term-paper-nfLWpm6z-py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Classifier(\n",
    "    glove_embs=glove_embs, \n",
    "    word2idx=vocab_word2idx, \n",
    "    idx2word=vocab_idx2word\n",
    ")\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "train(model, train_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "term-paper-nfLWpm6z-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
